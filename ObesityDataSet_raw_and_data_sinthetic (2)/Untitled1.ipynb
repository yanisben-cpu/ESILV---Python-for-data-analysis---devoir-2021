{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import mpld3\n",
    "mpld3.enable_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"ObesityDataSet_raw_and_data_sinthetic.csv\", sep = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Height</th>\n",
       "      <th>Weight</th>\n",
       "      <th>family_history_with_overweight</th>\n",
       "      <th>FAVC</th>\n",
       "      <th>FCVC</th>\n",
       "      <th>NCP</th>\n",
       "      <th>CAEC</th>\n",
       "      <th>SMOKE</th>\n",
       "      <th>CH2O</th>\n",
       "      <th>SCC</th>\n",
       "      <th>FAF</th>\n",
       "      <th>TUE</th>\n",
       "      <th>CALC</th>\n",
       "      <th>MTRANS</th>\n",
       "      <th>NObeyesdad</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Female</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>1.620000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Sometimes</td>\n",
       "      <td>no</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>no</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>no</td>\n",
       "      <td>Public_Transportation</td>\n",
       "      <td>Normal_Weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Female</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>1.520000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Sometimes</td>\n",
       "      <td>yes</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>yes</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Sometimes</td>\n",
       "      <td>Public_Transportation</td>\n",
       "      <td>Normal_Weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Male</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Sometimes</td>\n",
       "      <td>no</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>no</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>Frequently</td>\n",
       "      <td>Public_Transportation</td>\n",
       "      <td>Normal_Weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Male</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>87.000000</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Sometimes</td>\n",
       "      <td>no</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>no</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Frequently</td>\n",
       "      <td>Walking</td>\n",
       "      <td>Overweight_Level_I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Male</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>1.780000</td>\n",
       "      <td>89.800000</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Sometimes</td>\n",
       "      <td>no</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>no</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Sometimes</td>\n",
       "      <td>Public_Transportation</td>\n",
       "      <td>Overweight_Level_II</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2106</th>\n",
       "      <td>Female</td>\n",
       "      <td>20.976842</td>\n",
       "      <td>1.710730</td>\n",
       "      <td>131.408528</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Sometimes</td>\n",
       "      <td>no</td>\n",
       "      <td>1.728139</td>\n",
       "      <td>no</td>\n",
       "      <td>1.676269</td>\n",
       "      <td>0.906247</td>\n",
       "      <td>Sometimes</td>\n",
       "      <td>Public_Transportation</td>\n",
       "      <td>Obesity_Type_III</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2107</th>\n",
       "      <td>Female</td>\n",
       "      <td>21.982942</td>\n",
       "      <td>1.748584</td>\n",
       "      <td>133.742943</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Sometimes</td>\n",
       "      <td>no</td>\n",
       "      <td>2.005130</td>\n",
       "      <td>no</td>\n",
       "      <td>1.341390</td>\n",
       "      <td>0.599270</td>\n",
       "      <td>Sometimes</td>\n",
       "      <td>Public_Transportation</td>\n",
       "      <td>Obesity_Type_III</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2108</th>\n",
       "      <td>Female</td>\n",
       "      <td>22.524036</td>\n",
       "      <td>1.752206</td>\n",
       "      <td>133.689352</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Sometimes</td>\n",
       "      <td>no</td>\n",
       "      <td>2.054193</td>\n",
       "      <td>no</td>\n",
       "      <td>1.414209</td>\n",
       "      <td>0.646288</td>\n",
       "      <td>Sometimes</td>\n",
       "      <td>Public_Transportation</td>\n",
       "      <td>Obesity_Type_III</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2109</th>\n",
       "      <td>Female</td>\n",
       "      <td>24.361936</td>\n",
       "      <td>1.739450</td>\n",
       "      <td>133.346641</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Sometimes</td>\n",
       "      <td>no</td>\n",
       "      <td>2.852339</td>\n",
       "      <td>no</td>\n",
       "      <td>1.139107</td>\n",
       "      <td>0.586035</td>\n",
       "      <td>Sometimes</td>\n",
       "      <td>Public_Transportation</td>\n",
       "      <td>Obesity_Type_III</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2110</th>\n",
       "      <td>Female</td>\n",
       "      <td>23.664709</td>\n",
       "      <td>1.738836</td>\n",
       "      <td>133.472641</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Sometimes</td>\n",
       "      <td>no</td>\n",
       "      <td>2.863513</td>\n",
       "      <td>no</td>\n",
       "      <td>1.026452</td>\n",
       "      <td>0.714137</td>\n",
       "      <td>Sometimes</td>\n",
       "      <td>Public_Transportation</td>\n",
       "      <td>Obesity_Type_III</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2111 rows Ã— 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Gender        Age    Height      Weight family_history_with_overweight  \\\n",
       "0     Female  21.000000  1.620000   64.000000                            yes   \n",
       "1     Female  21.000000  1.520000   56.000000                            yes   \n",
       "2       Male  23.000000  1.800000   77.000000                            yes   \n",
       "3       Male  27.000000  1.800000   87.000000                             no   \n",
       "4       Male  22.000000  1.780000   89.800000                             no   \n",
       "...      ...        ...       ...         ...                            ...   \n",
       "2106  Female  20.976842  1.710730  131.408528                            yes   \n",
       "2107  Female  21.982942  1.748584  133.742943                            yes   \n",
       "2108  Female  22.524036  1.752206  133.689352                            yes   \n",
       "2109  Female  24.361936  1.739450  133.346641                            yes   \n",
       "2110  Female  23.664709  1.738836  133.472641                            yes   \n",
       "\n",
       "     FAVC  FCVC  NCP       CAEC SMOKE      CH2O  SCC       FAF       TUE  \\\n",
       "0      no   2.0  3.0  Sometimes    no  2.000000   no  0.000000  1.000000   \n",
       "1      no   3.0  3.0  Sometimes   yes  3.000000  yes  3.000000  0.000000   \n",
       "2      no   2.0  3.0  Sometimes    no  2.000000   no  2.000000  1.000000   \n",
       "3      no   3.0  3.0  Sometimes    no  2.000000   no  2.000000  0.000000   \n",
       "4      no   2.0  1.0  Sometimes    no  2.000000   no  0.000000  0.000000   \n",
       "...   ...   ...  ...        ...   ...       ...  ...       ...       ...   \n",
       "2106  yes   3.0  3.0  Sometimes    no  1.728139   no  1.676269  0.906247   \n",
       "2107  yes   3.0  3.0  Sometimes    no  2.005130   no  1.341390  0.599270   \n",
       "2108  yes   3.0  3.0  Sometimes    no  2.054193   no  1.414209  0.646288   \n",
       "2109  yes   3.0  3.0  Sometimes    no  2.852339   no  1.139107  0.586035   \n",
       "2110  yes   3.0  3.0  Sometimes    no  2.863513   no  1.026452  0.714137   \n",
       "\n",
       "            CALC                 MTRANS           NObeyesdad  \n",
       "0             no  Public_Transportation        Normal_Weight  \n",
       "1      Sometimes  Public_Transportation        Normal_Weight  \n",
       "2     Frequently  Public_Transportation        Normal_Weight  \n",
       "3     Frequently                Walking   Overweight_Level_I  \n",
       "4      Sometimes  Public_Transportation  Overweight_Level_II  \n",
       "...          ...                    ...                  ...  \n",
       "2106   Sometimes  Public_Transportation     Obesity_Type_III  \n",
       "2107   Sometimes  Public_Transportation     Obesity_Type_III  \n",
       "2108   Sometimes  Public_Transportation     Obesity_Type_III  \n",
       "2109   Sometimes  Public_Transportation     Obesity_Type_III  \n",
       "2110   Sometimes  Public_Transportation     Obesity_Type_III  \n",
       "\n",
       "[2111 rows x 17 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing age type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Height</th>\n",
       "      <th>Weight</th>\n",
       "      <th>family_history_with_overweight</th>\n",
       "      <th>FAVC</th>\n",
       "      <th>FCVC</th>\n",
       "      <th>NCP</th>\n",
       "      <th>CAEC</th>\n",
       "      <th>SMOKE</th>\n",
       "      <th>CH2O</th>\n",
       "      <th>SCC</th>\n",
       "      <th>FAF</th>\n",
       "      <th>TUE</th>\n",
       "      <th>CALC</th>\n",
       "      <th>MTRANS</th>\n",
       "      <th>NObeyesdad</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Female</td>\n",
       "      <td>21</td>\n",
       "      <td>1.620000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Sometimes</td>\n",
       "      <td>no</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>no</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>no</td>\n",
       "      <td>Public_Transportation</td>\n",
       "      <td>Normal_Weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Female</td>\n",
       "      <td>21</td>\n",
       "      <td>1.520000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Sometimes</td>\n",
       "      <td>yes</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>yes</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Sometimes</td>\n",
       "      <td>Public_Transportation</td>\n",
       "      <td>Normal_Weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Male</td>\n",
       "      <td>23</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Sometimes</td>\n",
       "      <td>no</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>no</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>Frequently</td>\n",
       "      <td>Public_Transportation</td>\n",
       "      <td>Normal_Weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Male</td>\n",
       "      <td>27</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>87.000000</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Sometimes</td>\n",
       "      <td>no</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>no</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Frequently</td>\n",
       "      <td>Walking</td>\n",
       "      <td>Overweight_Level_I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Male</td>\n",
       "      <td>22</td>\n",
       "      <td>1.780000</td>\n",
       "      <td>89.800000</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Sometimes</td>\n",
       "      <td>no</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>no</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Sometimes</td>\n",
       "      <td>Public_Transportation</td>\n",
       "      <td>Overweight_Level_II</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2106</th>\n",
       "      <td>Female</td>\n",
       "      <td>20</td>\n",
       "      <td>1.710730</td>\n",
       "      <td>131.408528</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Sometimes</td>\n",
       "      <td>no</td>\n",
       "      <td>1.728139</td>\n",
       "      <td>no</td>\n",
       "      <td>1.676269</td>\n",
       "      <td>0.906247</td>\n",
       "      <td>Sometimes</td>\n",
       "      <td>Public_Transportation</td>\n",
       "      <td>Obesity_Type_III</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2107</th>\n",
       "      <td>Female</td>\n",
       "      <td>21</td>\n",
       "      <td>1.748584</td>\n",
       "      <td>133.742943</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Sometimes</td>\n",
       "      <td>no</td>\n",
       "      <td>2.005130</td>\n",
       "      <td>no</td>\n",
       "      <td>1.341390</td>\n",
       "      <td>0.599270</td>\n",
       "      <td>Sometimes</td>\n",
       "      <td>Public_Transportation</td>\n",
       "      <td>Obesity_Type_III</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2108</th>\n",
       "      <td>Female</td>\n",
       "      <td>22</td>\n",
       "      <td>1.752206</td>\n",
       "      <td>133.689352</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Sometimes</td>\n",
       "      <td>no</td>\n",
       "      <td>2.054193</td>\n",
       "      <td>no</td>\n",
       "      <td>1.414209</td>\n",
       "      <td>0.646288</td>\n",
       "      <td>Sometimes</td>\n",
       "      <td>Public_Transportation</td>\n",
       "      <td>Obesity_Type_III</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2109</th>\n",
       "      <td>Female</td>\n",
       "      <td>24</td>\n",
       "      <td>1.739450</td>\n",
       "      <td>133.346641</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Sometimes</td>\n",
       "      <td>no</td>\n",
       "      <td>2.852339</td>\n",
       "      <td>no</td>\n",
       "      <td>1.139107</td>\n",
       "      <td>0.586035</td>\n",
       "      <td>Sometimes</td>\n",
       "      <td>Public_Transportation</td>\n",
       "      <td>Obesity_Type_III</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2110</th>\n",
       "      <td>Female</td>\n",
       "      <td>23</td>\n",
       "      <td>1.738836</td>\n",
       "      <td>133.472641</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Sometimes</td>\n",
       "      <td>no</td>\n",
       "      <td>2.863513</td>\n",
       "      <td>no</td>\n",
       "      <td>1.026452</td>\n",
       "      <td>0.714137</td>\n",
       "      <td>Sometimes</td>\n",
       "      <td>Public_Transportation</td>\n",
       "      <td>Obesity_Type_III</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2111 rows Ã— 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Gender  Age    Height      Weight family_history_with_overweight FAVC  \\\n",
       "0     Female   21  1.620000   64.000000                            yes   no   \n",
       "1     Female   21  1.520000   56.000000                            yes   no   \n",
       "2       Male   23  1.800000   77.000000                            yes   no   \n",
       "3       Male   27  1.800000   87.000000                             no   no   \n",
       "4       Male   22  1.780000   89.800000                             no   no   \n",
       "...      ...  ...       ...         ...                            ...  ...   \n",
       "2106  Female   20  1.710730  131.408528                            yes  yes   \n",
       "2107  Female   21  1.748584  133.742943                            yes  yes   \n",
       "2108  Female   22  1.752206  133.689352                            yes  yes   \n",
       "2109  Female   24  1.739450  133.346641                            yes  yes   \n",
       "2110  Female   23  1.738836  133.472641                            yes  yes   \n",
       "\n",
       "      FCVC  NCP       CAEC SMOKE      CH2O  SCC       FAF       TUE  \\\n",
       "0      2.0  3.0  Sometimes    no  2.000000   no  0.000000  1.000000   \n",
       "1      3.0  3.0  Sometimes   yes  3.000000  yes  3.000000  0.000000   \n",
       "2      2.0  3.0  Sometimes    no  2.000000   no  2.000000  1.000000   \n",
       "3      3.0  3.0  Sometimes    no  2.000000   no  2.000000  0.000000   \n",
       "4      2.0  1.0  Sometimes    no  2.000000   no  0.000000  0.000000   \n",
       "...    ...  ...        ...   ...       ...  ...       ...       ...   \n",
       "2106   3.0  3.0  Sometimes    no  1.728139   no  1.676269  0.906247   \n",
       "2107   3.0  3.0  Sometimes    no  2.005130   no  1.341390  0.599270   \n",
       "2108   3.0  3.0  Sometimes    no  2.054193   no  1.414209  0.646288   \n",
       "2109   3.0  3.0  Sometimes    no  2.852339   no  1.139107  0.586035   \n",
       "2110   3.0  3.0  Sometimes    no  2.863513   no  1.026452  0.714137   \n",
       "\n",
       "            CALC                 MTRANS           NObeyesdad  \n",
       "0             no  Public_Transportation        Normal_Weight  \n",
       "1      Sometimes  Public_Transportation        Normal_Weight  \n",
       "2     Frequently  Public_Transportation        Normal_Weight  \n",
       "3     Frequently                Walking   Overweight_Level_I  \n",
       "4      Sometimes  Public_Transportation  Overweight_Level_II  \n",
       "...          ...                    ...                  ...  \n",
       "2106   Sometimes  Public_Transportation     Obesity_Type_III  \n",
       "2107   Sometimes  Public_Transportation     Obesity_Type_III  \n",
       "2108   Sometimes  Public_Transportation     Obesity_Type_III  \n",
       "2109   Sometimes  Public_Transportation     Obesity_Type_III  \n",
       "2110   Sometimes  Public_Transportation     Obesity_Type_III  \n",
       "\n",
       "[2111 rows x 17 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset2 = dataset\n",
    "dataset2.Age = dataset2.Age.astype(int)\n",
    "dataset2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Gender', 'Age', 'Height', 'Weight', 'family_history_with_overweight',\n",
       "       'FAVC', 'FCVC', 'NCP', 'CAEC', 'SMOKE', 'CH2O', 'SCC', 'FAF', 'TUE',\n",
       "       'CALC', 'MTRANS', 'NObeyesdad'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset2.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\yYaNn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\yYaNn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "import boto3\n",
    "from io import StringIO\n",
    "import nltk\n",
    "#import unidecode\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords \n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "#import tqdm\n",
    "from tqdm import tqdm\n",
    "#from unidecode import unidecode\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import joblib\n",
    "import boto3\n",
    "import tempfile\n",
    "\n",
    "import pandas\n",
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets, linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#y = dataset2[dataset2['CAEC'].notna()].NObeyesdad # define the target variable (dependent variable) as y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorizer_word = TfidfVectorizer(analyzer='word',ngram_range=(1,2),min_df = 5, max_df = 0.5) # fit at Word level on all\n",
    "#vectorizer_word.fit(dataset2.CAEC)\n",
    "#\n",
    "#vectorizer_char = TfidfVectorizer(analyzer='char',ngram_range=(1,4),min_df = 5, max_df = 0.5) # fit at Word level on all\n",
    "#vectorizer_char.fit(dataset2.CAEC)\n",
    "#\n",
    "#tfidf_matrix_word_train = vectorizer_word.transform(dataset2[dataset2.Gender.notna()].NObeyesdad) # transfrom on train\n",
    "#tfidf_matrix_char_train = vectorizer_char.transform(dataset2[dataset2.Gender.notna()].NObeyesdad) # transfrom on train\n",
    "#\n",
    "#tfidf_matrix_word_char_train = hstack((tfidf_matrix_word_train, tfidf_matrix_char_train)) # stack matrixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train, X_test, y_train, y_test = train_test_split(tfidf_matrix_word_char_train, y, test_size = 0.3) #0.33\n",
    "#lrm = LogisticRegression(solver='newton-cg')\n",
    "#model = lrm.fit(X_train, y_train)\n",
    "#100*model.score(X = X_test, y = y_test)\n",
    "##predictions = lrm.predict(X_test)\n",
    "##predictions.score(tfidf_matrix_word_char_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ALT : Models' bridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# vars on which we will split data\n",
    "X = dataset[dataset.CAEC.notna()].CAEC\n",
    "y = dataset[dataset.CAEC.notna()].NObeyesdad # define the target variable (dependent variable) as y\n",
    "\n",
    "# defining a starting time to see later how much time the process will take\n",
    "start = time.time()\n",
    "\n",
    "# splitting data\n",
    "x_train_k, x_test_k, y_train_k, y_test_k = train_test_split(X, y, test_size = 0.3)\n",
    "\n",
    "vectorizer_word = TfidfVectorizer(min_df = 5, max_df = 0.5, analyzer = 'word', ngram_range = (1, 2))\n",
    "vectorizer_char = TfidfVectorizer(min_df = 5, max_df = 0.5, analyzer = 'char', ngram_range = (1, 4))\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df = 5, max_df = 0.5, ngram_range = (1, 4))\n",
    "\n",
    "vectorizer_word.fit(x_train_k)\n",
    "vectorizer_char.fit(x_train_k)\n",
    "\n",
    "tfidf_matrix_word_train = vectorizer_word.transform(x_train_k)\n",
    "tfidf_matrix_word_test = vectorizer_word.transform(x_test_k)\n",
    "\n",
    "tfidf_matrix_char_train = vectorizer_char.transform(x_train_k)\n",
    "tfidf_matrix_char_test = vectorizer_char.transform(x_test_k)\n",
    "\n",
    "tfidf_matrix_word_char_train =  hstack((tfidf_matrix_word_train, tfidf_matrix_char_train))\n",
    "tfidf_matrix_word_char_test =  hstack((tfidf_matrix_word_test, tfidf_matrix_char_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Gender', 'Age', 'Height', 'Weight', 'family_history_with_overweight',\n",
       "       'FAVC', 'FCVC', 'NCP', 'CAEC', 'SMOKE', 'CH2O', 'SCC', 'FAF', 'TUE',\n",
       "       'CALC', 'MTRANS', 'NObeyesdad'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.555205047318612\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>Insufficient_Weight</th>\n",
       "      <th>Normal_Weight</th>\n",
       "      <th>Obesity_Type_I</th>\n",
       "      <th>Overweight_Level_I</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NObeyesdad</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Insufficient_Weight</th>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Normal_Weight</th>\n",
       "      <td>28</td>\n",
       "      <td>11</td>\n",
       "      <td>48</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Obesity_Type_I</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>86</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Obesity_Type_II</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>96</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Obesity_Type_III</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>85</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Overweight_Level_I</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>76</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Overweight_Level_II</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>94</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0                Insufficient_Weight  Normal_Weight  Obesity_Type_I  \\\n",
       "NObeyesdad                                                                \n",
       "Insufficient_Weight                   35              0              41   \n",
       "Normal_Weight                         28             11              48   \n",
       "Obesity_Type_I                         1              2              86   \n",
       "Obesity_Type_II                        0              0              96   \n",
       "Obesity_Type_III                       1              0              85   \n",
       "Overweight_Level_I                     3              1              76   \n",
       "Overweight_Level_II                    8              0              94   \n",
       "\n",
       "col_0                Overweight_Level_I  \n",
       "NObeyesdad                               \n",
       "Insufficient_Weight                   0  \n",
       "Normal_Weight                         4  \n",
       "Obesity_Type_I                        1  \n",
       "Obesity_Type_II                       1  \n",
       "Obesity_Type_III                      0  \n",
       "Overweight_Level_I                   11  \n",
       "Overweight_Level_II                   1  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# vars on which we will split data\n",
    "X = dataset[dataset.CAEC.notna()].CAEC\n",
    "y = dataset[dataset.CAEC.notna()].NObeyesdad # define the target variable (dependent variable) as y\n",
    "\n",
    "# defining a starting time to see later how much time the process will take\n",
    "start = time.time()\n",
    "\n",
    "# splitting data\n",
    "x_train_k, x_test_k, y_train_k, y_test_k = train_test_split(X, y, test_size = 0.3)\n",
    "\n",
    "vectorizer_word = TfidfVectorizer(min_df = 5, max_df = 0.5, analyzer = 'word', ngram_range = (1, 2))\n",
    "vectorizer_char = TfidfVectorizer(min_df = 5, max_df = 0.5, analyzer = 'char', ngram_range = (1, 4))\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df = 5, max_df = 0.5, ngram_range = (1, 4))\n",
    "\n",
    "vectorizer_word.fit(x_train_k)\n",
    "vectorizer_char.fit(x_train_k)\n",
    "\n",
    "tfidf_matrix_word_train = vectorizer_word.transform(x_train_k)\n",
    "tfidf_matrix_word_test = vectorizer_word.transform(x_test_k)\n",
    "\n",
    "tfidf_matrix_char_train = vectorizer_char.transform(x_train_k)\n",
    "tfidf_matrix_char_test = vectorizer_char.transform(x_test_k)\n",
    "\n",
    "tfidf_matrix_word_char_train =  hstack((tfidf_matrix_word_train, tfidf_matrix_char_train))\n",
    "tfidf_matrix_word_char_test =  hstack((tfidf_matrix_word_test, tfidf_matrix_char_test))\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators = 100)\n",
    "rf.fit(tfidf_matrix_word_char_train, y_train_k)\n",
    "y_pred_rf = rf.predict(tfidf_matrix_word_char_test)\n",
    "\n",
    "print(100*rf.score(tfidf_matrix_word_char_test,y_test_k))\n",
    "\n",
    "pd.crosstab(y_test_k, y_pred_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.76340694006309\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>Insufficient_Weight</th>\n",
       "      <th>Normal_Weight</th>\n",
       "      <th>Obesity_Type_I</th>\n",
       "      <th>Overweight_Level_I</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NObeyesdad</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Insufficient_Weight</th>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Normal_Weight</th>\n",
       "      <td>33</td>\n",
       "      <td>7</td>\n",
       "      <td>46</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Obesity_Type_I</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>106</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Obesity_Type_II</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>79</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Obesity_Type_III</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>96</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Overweight_Level_I</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>74</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Overweight_Level_II</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0                Insufficient_Weight  Normal_Weight  Obesity_Type_I  \\\n",
       "NObeyesdad                                                                \n",
       "Insufficient_Weight                   32              0              43   \n",
       "Normal_Weight                         33              7              46   \n",
       "Obesity_Type_I                         2              0             106   \n",
       "Obesity_Type_II                        0              0              79   \n",
       "Obesity_Type_III                       0              0              96   \n",
       "Overweight_Level_I                     3              2              74   \n",
       "Overweight_Level_II                    6              0              88   \n",
       "\n",
       "col_0                Overweight_Level_I  \n",
       "NObeyesdad                               \n",
       "Insufficient_Weight                   0  \n",
       "Normal_Weight                         3  \n",
       "Obesity_Type_I                        1  \n",
       "Obesity_Type_II                       0  \n",
       "Obesity_Type_III                      0  \n",
       "Overweight_Level_I                   12  \n",
       "Overweight_Level_II                   1  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# vars on which we will split data\n",
    "X = dataset[dataset.CAEC.notna()].CAEC\n",
    "y = dataset[dataset.CAEC.notna()].NObeyesdad # define the target variable (dependent variable) as y\n",
    "\n",
    "# defining a starting time to see later how much time the process will take\n",
    "start = time.time()\n",
    "\n",
    "# splitting data\n",
    "x_train_k, x_test_k, y_train_k, y_test_k = train_test_split(X, y, test_size = 0.3)\n",
    "\n",
    "vectorizer_word = TfidfVectorizer(min_df = 5, max_df = 0.5, analyzer = 'word', ngram_range = (1, 2))\n",
    "vectorizer_char = TfidfVectorizer(min_df = 5, max_df = 0.5, analyzer = 'char', ngram_range = (1, 4))\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df = 5, max_df = 0.5, ngram_range = (1, 4))\n",
    "\n",
    "vectorizer_word.fit(x_train_k)\n",
    "vectorizer_char.fit(x_train_k)\n",
    "\n",
    "tfidf_matrix_word_train = vectorizer_word.transform(x_train_k)\n",
    "tfidf_matrix_word_test = vectorizer_word.transform(x_test_k)\n",
    "\n",
    "tfidf_matrix_char_train = vectorizer_char.transform(x_train_k)\n",
    "tfidf_matrix_char_test = vectorizer_char.transform(x_test_k)\n",
    "\n",
    "tfidf_matrix_word_char_train =  hstack((tfidf_matrix_word_train, tfidf_matrix_char_train))\n",
    "tfidf_matrix_word_char_test =  hstack((tfidf_matrix_word_test, tfidf_matrix_char_test))\n",
    "\n",
    "svm = SVC()\n",
    "svm.fit(tfidf_matrix_word_char_train, y_train_k)\n",
    "y_pred_svm = svm.predict(tfidf_matrix_word_char_test)\n",
    "print(100*svm.score(tfidf_matrix_word_char_test,y_test_k))\n",
    "\n",
    "pd.crosstab(y_test_k, y_pred_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reg log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy : 24.7634  \n",
      "Time spent : 0.12267231941223145\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# vars on which we will split data\n",
    "X = dataset[dataset.CAEC.notna()].CAEC\n",
    "y = dataset[dataset.CAEC.notna()].NObeyesdad # define the target variable (dependent variable) as y\n",
    "\n",
    "# defining a starting time to see later how much time the process will take\n",
    "start = time.time()\n",
    "\n",
    "# splitting data\n",
    "x_train_k, x_test_k, y_train_k, y_test_k = train_test_split(X, y, test_size = 0.3)\n",
    "\n",
    "vectorizer_word = TfidfVectorizer(min_df = 5, max_df = 0.5, analyzer = 'word', ngram_range = (1, 2))\n",
    "vectorizer_char = TfidfVectorizer(min_df = 5, max_df = 0.5, analyzer = 'char', ngram_range = (1, 4))\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df = 5, max_df = 0.5, ngram_range = (1, 4))\n",
    "\n",
    "vectorizer_word.fit(x_train_k)\n",
    "vectorizer_char.fit(x_train_k)\n",
    "\n",
    "tfidf_matrix_word_train = vectorizer_word.transform(x_train_k)\n",
    "tfidf_matrix_word_test = vectorizer_word.transform(x_test_k)\n",
    "\n",
    "tfidf_matrix_char_train = vectorizer_char.transform(x_train_k)\n",
    "tfidf_matrix_char_test = vectorizer_char.transform(x_test_k)\n",
    "\n",
    "tfidf_matrix_word_char_train =  hstack((tfidf_matrix_word_train, tfidf_matrix_char_train))\n",
    "tfidf_matrix_word_char_test =  hstack((tfidf_matrix_word_test, tfidf_matrix_char_test))\n",
    "\n",
    "lrm = LogisticRegression(solver = 'newton-cg')\n",
    "\n",
    "lrm.fit(tfidf_matrix_word_char_train, y_train_k)\n",
    "\n",
    "y_pred_word_char = lrm.predict(tfidf_matrix_word_char_test)\n",
    "prec = 100 * lrm.score(tfidf_matrix_word_char_test,y_test_k)\n",
    "\n",
    "temps = time.time() - start\n",
    "\n",
    "print('\\nAccuracy :', np.round(prec,4), ' \\nTime spent :', temps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy : 25.0789  \n",
      "Time spent : 0.11668682098388672\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# vars on which we will split data\n",
    "X = dataset[dataset.CAEC.notna()].CAEC\n",
    "y = dataset[dataset.CAEC.notna()].NObeyesdad # define the target variable (dependent variable) as y\n",
    "\n",
    "# defining a starting time to see later how much time the process will take\n",
    "start = time.time()\n",
    "\n",
    "# splitting data\n",
    "x_train_k, x_test_k, y_train_k, y_test_k = train_test_split(X, y, test_size = 0.3)\n",
    "\n",
    "vectorizer_word = TfidfVectorizer(min_df = 5, max_df = 0.5, analyzer = 'word', ngram_range = (1, 2))\n",
    "vectorizer_char = TfidfVectorizer(min_df = 5, max_df = 0.5, analyzer = 'char', ngram_range = (1, 4))\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df = 5, max_df = 0.5, ngram_range = (1, 4))\n",
    "\n",
    "vectorizer_word.fit(x_train_k)\n",
    "vectorizer_char.fit(x_train_k)\n",
    "\n",
    "tfidf_matrix_word_train = vectorizer_word.transform(x_train_k)\n",
    "tfidf_matrix_word_test = vectorizer_word.transform(x_test_k)\n",
    "\n",
    "tfidf_matrix_char_train = vectorizer_char.transform(x_train_k)\n",
    "tfidf_matrix_char_test = vectorizer_char.transform(x_test_k)\n",
    "\n",
    "tfidf_matrix_word_char_train =  hstack((tfidf_matrix_word_train, tfidf_matrix_char_train))\n",
    "tfidf_matrix_word_char_test =  hstack((tfidf_matrix_word_test, tfidf_matrix_char_test))\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "lbg = BaggingClassifier()\n",
    "\n",
    "lbg.fit(tfidf_matrix_word_char_train, y_train_k)\n",
    "\n",
    "y_pred_word_char = lbg.predict(tfidf_matrix_word_char_test)\n",
    "prec = 100 * lbg.score(tfidf_matrix_word_char_test,y_test_k)\n",
    "\n",
    "temps = time.time() - start\n",
    "\n",
    "print('\\nAccuracy :', np.round(prec,4), ' \\nTime spent :', temps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy : 22.3975  \n",
      "Time spent : 0.3859999179840088\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# vars on which we will split data\n",
    "X = dataset[dataset.CAEC.notna()].CAEC\n",
    "y = dataset[dataset.CAEC.notna()].NObeyesdad # define the target variable (dependent variable) as y\n",
    "\n",
    "# defining a starting time to see later how much time the process will take\n",
    "start = time.time()\n",
    "\n",
    "# splitting data\n",
    "x_train_k, x_test_k, y_train_k, y_test_k = train_test_split(X, y, test_size = 0.3)\n",
    "\n",
    "vectorizer_word = TfidfVectorizer(min_df = 5, max_df = 0.5, analyzer = 'word', ngram_range = (1, 2))\n",
    "vectorizer_char = TfidfVectorizer(min_df = 5, max_df = 0.5, analyzer = 'char', ngram_range = (1, 4))\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df = 5, max_df = 0.5, ngram_range = (1, 4))\n",
    "\n",
    "vectorizer_word.fit(x_train_k)\n",
    "vectorizer_char.fit(x_train_k)\n",
    "\n",
    "tfidf_matrix_word_train = vectorizer_word.transform(x_train_k)\n",
    "tfidf_matrix_word_test = vectorizer_word.transform(x_test_k)\n",
    "\n",
    "tfidf_matrix_char_train = vectorizer_char.transform(x_train_k)\n",
    "tfidf_matrix_char_test = vectorizer_char.transform(x_test_k)\n",
    "\n",
    "tfidf_matrix_word_char_train =  hstack((tfidf_matrix_word_train, tfidf_matrix_char_train))\n",
    "tfidf_matrix_word_char_test =  hstack((tfidf_matrix_word_test, tfidf_matrix_char_test))\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "lbt_ada = AdaBoostClassifier(n_estimators=100, random_state=16)\n",
    "\n",
    "lbt_ada.fit(tfidf_matrix_word_char_train, y_train_k)\n",
    "\n",
    "y_pred_word_char = lbt_ada.predict(tfidf_matrix_word_char_test)\n",
    "prec = 100 * lbt_ada.score(tfidf_matrix_word_char_test,y_test_k)\n",
    "\n",
    "temps = time.time() - start\n",
    "\n",
    "print('\\nAccuracy :', np.round(prec,4), ' \\nTime spent :', temps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy : 24.4479  \n",
      "Time spent : 0.7563085556030273\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# vars on which we will split data\n",
    "X = dataset[dataset.CAEC.notna()].CAEC\n",
    "y = dataset[dataset.CAEC.notna()].NObeyesdad # define the target variable (dependent variable) as y\n",
    "\n",
    "# defining a starting time to see later how much time the process will take\n",
    "start = time.time()\n",
    "\n",
    "# splitting data\n",
    "x_train_k, x_test_k, y_train_k, y_test_k = train_test_split(X, y, test_size = 0.3)\n",
    "\n",
    "vectorizer_word = TfidfVectorizer(min_df = 5, max_df = 0.5, analyzer = 'word', ngram_range = (1, 2))\n",
    "vectorizer_char = TfidfVectorizer(min_df = 5, max_df = 0.5, analyzer = 'char', ngram_range = (1, 4))\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df = 5, max_df = 0.5, ngram_range = (1, 4))\n",
    "\n",
    "vectorizer_word.fit(x_train_k)\n",
    "vectorizer_char.fit(x_train_k)\n",
    "\n",
    "tfidf_matrix_word_train = vectorizer_word.transform(x_train_k)\n",
    "tfidf_matrix_word_test = vectorizer_word.transform(x_test_k)\n",
    "\n",
    "tfidf_matrix_char_train = vectorizer_char.transform(x_train_k)\n",
    "tfidf_matrix_char_test = vectorizer_char.transform(x_test_k)\n",
    "\n",
    "tfidf_matrix_word_char_train =  hstack((tfidf_matrix_word_train, tfidf_matrix_char_train))\n",
    "tfidf_matrix_word_char_test =  hstack((tfidf_matrix_word_test, tfidf_matrix_char_test))\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "lbt_sgb = GradientBoostingClassifier(n_estimators=100)\n",
    "\n",
    "lbt_sgb.fit(tfidf_matrix_word_char_train, y_train_k)\n",
    "\n",
    "y_pred_word_char = lbt_sgb.predict(tfidf_matrix_word_char_test)\n",
    "prec = 100 * lbt_sgb.score(tfidf_matrix_word_char_test,y_test_k)\n",
    "\n",
    "temps = time.time() - start\n",
    "\n",
    "print('\\nAccuracy :', np.round(prec,4), ' \\nTime spent :', temps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Gender', 'Age', 'Height', 'Weight', 'family_history_with_overweight',\n",
       "       'FAVC', 'FCVC', 'NCP', 'CAEC', 'SMOKE', 'CH2O', 'SCC', 'FAF', 'TUE',\n",
       "       'CALC', 'MTRANS', 'NObeyesdad'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy : 30.9148  \n",
      "Time spent : 0.1625988483428955\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# vars on which we will split data\n",
    "X = dataset[dataset.CAEC.notna()].Gender\n",
    "y = dataset[dataset.CAEC.notna()].NObeyesdad # define the target variable (dependent variable) as y\n",
    "\n",
    "# defining a starting time to see later how much time the process will take\n",
    "start = time.time()\n",
    "\n",
    "# splitting data\n",
    "x_train_k, x_test_k, y_train_k, y_test_k = train_test_split(X, y, test_size = 0.3)\n",
    "\n",
    "vectorizer_word = TfidfVectorizer(min_df = 5, max_df = 0.5, analyzer = 'word', ngram_range = (1, 2))\n",
    "vectorizer_char = TfidfVectorizer(min_df = 5, max_df = 0.5, analyzer = 'char', ngram_range = (1, 4))\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df = 5, max_df = 0.5, ngram_range = (1, 4))\n",
    "\n",
    "vectorizer_word.fit(x_train_k)\n",
    "vectorizer_char.fit(x_train_k)\n",
    "\n",
    "tfidf_matrix_word_train = vectorizer_word.transform(x_train_k)\n",
    "tfidf_matrix_word_test = vectorizer_word.transform(x_test_k)\n",
    "\n",
    "tfidf_matrix_char_train = vectorizer_char.transform(x_train_k)\n",
    "tfidf_matrix_char_test = vectorizer_char.transform(x_test_k)\n",
    "\n",
    "tfidf_matrix_word_char_train =  hstack((tfidf_matrix_word_train, tfidf_matrix_char_train))\n",
    "tfidf_matrix_word_char_test =  hstack((tfidf_matrix_word_test, tfidf_matrix_char_test))\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "lbg = ExtraTreesClassifier(n_estimators=100)\n",
    "\n",
    "lbg.fit(tfidf_matrix_word_char_train, y_train_k)\n",
    "\n",
    "y_pred_word_char = lbg.predict(tfidf_matrix_word_char_test)\n",
    "prec = 100 * lbg.score(tfidf_matrix_word_char_test,y_test_k)\n",
    "\n",
    "temps = time.time() - start\n",
    "\n",
    "print('\\nAccuracy :', np.round(prec,4), ' \\nTime spent :', temps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
